{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QlWTvOkDNATx",
        "H-fYXBJ3N4Nq",
        "6pTUqy14yQS6",
        "bvR3HnQebo4N"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYwZ6J+UjzS/03pXgJK7ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blowmeaway1234/Artificial-Intelligence/blob/main/Midterm_Vietnam_Current_Recognition_build_with_ANN_using_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqrR222rMjkf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset Vietnamese Currency "
      ],
      "metadata": {
        "id": "QlWTvOkDNATx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/DataSetCurrency # test data "
      ],
      "metadata": {
        "id": "ZgPdaGdXM5Fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2e2cb1-31f3-49f2-bc0b-97f2173cbf10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'gdrive/MyDrive/DataSetCurrent': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Test and Train data\n",
        "! unzip gdrive/MyDrive/DataSetVNC/testingSet.zip\n",
        "! unzip gdrive/MyDrive/DataSetVNC/trainingSet.zip"
      ],
      "metadata": {
        "id": "yEr-aAkjM7Xn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cbd081-8965-4e0d-f6ca-bfbc0fdc3748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open gdrive/MyDrive/DataSetVNC/testingSet.zip, gdrive/MyDrive/DataSetVNC/testingSet.zip.zip or gdrive/MyDrive/DataSetVNC/testingSet.zip.ZIP.\n",
            "unzip:  cannot find or open gdrive/MyDrive/DataSetVNC/trainingSet.zip, gdrive/MyDrive/DataSetVNC/trainingSet.zip.zip or gdrive/MyDrive/DataSetVNC/trainingSet.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check traindata\n",
        "for i in range(10): # lệnh !ls không hiểu python nên ko dùng '{}'.format i\n",
        "    !ls trainingSet/{i}| wc -l"
      ],
      "metadata": {
        "id": "bXp4QuoJM-X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5931451f-354c-44d4-a696-678494cb9bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'trainingSet/0': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/1': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/2': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/3': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/4': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/5': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/6': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/7': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/8': No such file or directory\n",
            "0\n",
            "ls: cannot access 'trainingSet/9': No such file or directory\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check testdata\n",
        "!ls testingSet/| wc -l"
      ],
      "metadata": {
        "id": "JbAjqzyuNOR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b569b17-df8d-44e1-984d-c11d2600a81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'testingSet/': No such file or directory\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls trainingSet/| wc -l"
      ],
      "metadata": {
        "id": "iDEC_0_bNROG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67036473-a081-40d3-e9d8-49e23aaacc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'trainingSet/': No such file or directory\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "H-fYXBJ3N4Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "U5n7xtEeN_Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_root = Path('trainingSetVNC/')\n",
        "testing_root=Path('testingSetVNC/')\n",
        "# convert to string type\n",
        "filelist_ds= tf.data.Dataset.list_files(str(train_root/'*/*')) # 2 level scan (train_root to folders 0 - 9 then access each one)\n",
        "# 9 random elements\n",
        "for file in filelist_ds.take(4):\n",
        "  print(file) #filename path in tf.data"
      ],
      "metadata": {
        "id": "SpjPt6DuOCso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, '/')\n",
        "  #part[0] = train\n",
        "  #part[1] = 0/1.../9\n",
        "  #part[2] = filename\n",
        "  if parts[-2] == \"0\":\n",
        "    labels=[0,0,0,0,0,0,0,0,0,1]\n",
        "  elif parts[-2] == \"1\":\n",
        "    labels=[1,0,0,0,0,0,0,0,0,0]\n",
        "  elif parts[-2] == \"2\":\n",
        "    labels=[0,1,0,0,0,0,0,0,0,0]\n",
        "  elif parts[-2] == \"3\":\n",
        "    labels=[0,0,1,0,0,0,0,0,0,0]\n",
        "  elif parts[-2] == \"4\":\n",
        "    labels=[0,0,0,1,0,0,0,0,0,0]\n",
        "  elif parts[-2] == \"5\":\n",
        "    labels=[0,0,0,0,1,0,0,0,0,0]\n",
        "  elif parts[-2] == \"6\":\n",
        "    labels=[0,0,0,0,0,1,0,0,0,0]\n",
        "  elif parts[-2] == \"7\":\n",
        "    labels=[0,0,0,0,0,0,1,0,0,0]\n",
        "  elif parts[-2] == \"8\":\n",
        "    labels=[0,0,0,0,0,0,0,1,0,0]\n",
        "  elif parts[-2] == \"9\":\n",
        "    labels=[0,0,0,0,0,0,0,0,1,0]\n",
        "  else: \n",
        "    labels=[0,0,0,0,0,0,0,0,0,0]\n",
        "    #return labels value but have to convert to tensor becase string we use is python not tf.tensor\n",
        "  return tf.convert_to_tensor(labels) "
      ],
      "metadata": {
        "id": "Sd5nrMJeOKRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "for file in filelist_ds.take(9):\n",
        "  print('filename',file.numpy().decode('utf-8')) # utf8 bỏ tf.tensor\n",
        "  print('label:',get_label(file).numpy())"
      ],
      "metadata": {
        "id": "2fKlL8mcOUT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow._api.v2 import image\n",
        "# Preprocessing function\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "def preprocessing(file_path): #data we have just a file list but data for train model must be pixel value\n",
        "  # Read file\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = tf.image.decode_jpeg(img,channels=3) # 1 is gray, 3 is color; we use image function of tf.image ; jpeg because jpg is our flie\n",
        "  # Transform\n",
        "    # Conver from uint 8 to float 32 and Normalize value to [0,1]\n",
        "  img = tf.image.convert_image_dtype(img,tf.float32)\n",
        "    # Resize\n",
        "  img = tf.image.resize(img,[img_width,img_height])\n",
        "    # Get image label\n",
        "  label = get_label(file_path)\n",
        "  # Return\n",
        "  return img, label"
      ],
      "metadata": {
        "id": "GtPFvHlQOWx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_size = 25000 #(shuffle size) \n",
        "# Set parameter\n",
        "batch_size = 128\n",
        "# Build data\n",
        "train_ds = filelist_ds.shuffle(ds_size) # Shuffling the input and it doesn't have labels\n",
        "train_ds = train_ds.map(preprocessing,num_parallel_calls=tf.data.AUTOTUNE) # Process each element + get labels => train_ds = pixel val and labels\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "train_ds = train_ds.prefetch(8) # chia luồn data"
      ],
      "metadata": {
        "id": "Th_YLiWAOaHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "6pTUqy14yQS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model \n",
        "# Load VGG model\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',\n",
        "    input_shape=(180,180,3),\n",
        "    include_top = False, # remove classifier output\n",
        ")\n",
        "base_model.trainable = False # No retrain\n",
        "# Have 10 class (0,..9)\n",
        "number_of_classes =10  \n",
        "# Create model with functional API\n",
        "inputs = tf.keras.Input(shape=(180,180,3))\n",
        "x = base_model(inputs, training=False)\n",
        "# Add more declaration \n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x) # include_top is remove (output remove) -> create output\n",
        "# Outputs Dense 10 classes\n",
        "outputs = tf.keras.layers.Dense(number_of_classes,activation='softmax')(x)\n",
        "# Creat model\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "# Show model\n",
        "tf.keras.utils.plot_model(model,'MNIST model.png',show_shapes=True)"
      ],
      "metadata": {
        "id": "pK1rQvz7Y5Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and training model"
      ],
      "metadata": {
        "id": "bvR3HnQebo4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=[\"accuracy\"] #caculate accuracy on each batch\n",
        ")"
      ],
      "metadata": {
        "id": "9pJ9RXnPPwPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(train_ds, epochs = 10)"
      ],
      "metadata": {
        "id": "psPfNv8GSM9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('VietnamCurrentANNmodel.h5')"
      ],
      "metadata": {
        "id": "QQ7iRqsQSXF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}